{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Data')\n",
    "pkl_file = open('isochrones.pkl', 'rb')\n",
    "stacked_isochrones = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('columns.pkl', 'rb')\n",
    "x_columns = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('x_values.pkl', 'rb')\n",
    "x_values = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('isoc_cols.pkl', 'rb')\n",
    "isoc_columns = pickle.load(pkl_file)\n",
    "\n",
    "x_input=pd.read_csv('x_input')\n",
    "x_input_err=pd.read_csv('x_input_err')\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ra_error</th>\n",
       "      <th>dec_error</th>\n",
       "      <th>parallax_error</th>\n",
       "      <th>phot_g_mean_mag_error</th>\n",
       "      <th>bp_rp_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.049640</td>\n",
       "      <td>0.053262</td>\n",
       "      <td>0.068056</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.007693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.042647</td>\n",
       "      <td>0.041917</td>\n",
       "      <td>0.053403</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>0.006713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.045167</td>\n",
       "      <td>0.044909</td>\n",
       "      <td>0.052555</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>0.007003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.064142</td>\n",
       "      <td>0.066674</td>\n",
       "      <td>0.092999</td>\n",
       "      <td>0.003026</td>\n",
       "      <td>0.010077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.021657</td>\n",
       "      <td>0.020979</td>\n",
       "      <td>0.025863</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>0.007257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>846</td>\n",
       "      <td>0.046587</td>\n",
       "      <td>0.046271</td>\n",
       "      <td>0.058075</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>0.006425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>847</td>\n",
       "      <td>0.016018</td>\n",
       "      <td>0.016901</td>\n",
       "      <td>0.021350</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.004902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>848</td>\n",
       "      <td>0.064591</td>\n",
       "      <td>0.069871</td>\n",
       "      <td>0.086673</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.012391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>849</td>\n",
       "      <td>0.087949</td>\n",
       "      <td>0.103732</td>\n",
       "      <td>0.129928</td>\n",
       "      <td>0.003084</td>\n",
       "      <td>0.016993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>850</td>\n",
       "      <td>0.111580</td>\n",
       "      <td>0.118203</td>\n",
       "      <td>0.154710</td>\n",
       "      <td>0.003244</td>\n",
       "      <td>0.016407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>851 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  ra_error  dec_error  parallax_error  phot_g_mean_mag_error  \\\n",
       "0             0  0.049640   0.053262        0.068056               0.002859   \n",
       "1             1  0.042647   0.041917        0.053403               0.002824   \n",
       "2             2  0.045167   0.044909        0.052555               0.002824   \n",
       "3             3  0.064142   0.066674        0.092999               0.003026   \n",
       "4             4  0.021657   0.020979        0.025863               0.002785   \n",
       "..          ...       ...        ...             ...                    ...   \n",
       "846         846  0.046587   0.046271        0.058075               0.002818   \n",
       "847         847  0.016018   0.016901        0.021350               0.002772   \n",
       "848         848  0.064591   0.069871        0.086673               0.002890   \n",
       "849         849  0.087949   0.103732        0.129928               0.003084   \n",
       "850         850  0.111580   0.118203        0.154710               0.003244   \n",
       "\n",
       "     bp_rp_error  \n",
       "0       0.007693  \n",
       "1       0.006713  \n",
       "2       0.007003  \n",
       "3       0.010077  \n",
       "4       0.007257  \n",
       "..           ...  \n",
       "846     0.006425  \n",
       "847     0.004902  \n",
       "848     0.012391  \n",
       "849     0.016993  \n",
       "850     0.016407  \n",
       "\n",
       "[851 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    #array = np.asarray(array)\n",
    "    idx = (torch.abs(array - value)).argmin()\n",
    "    return array[idx],idx\n",
    "\n",
    "def isochrone_selector(feh,age):\n",
    "    '''if(feh<-4 or feh>0.5):\n",
    "        raise NotImplementedError\n",
    "    if(age<5 or age>10.3):\n",
    "        raise NotImplementedError\n",
    "    else:'''\n",
    "    logagegrid = torch.tensor(np.linspace(5,10.3,105))\n",
    "    fehgrid = torch.tensor(np.linspace(-4,0.5,90))\n",
    "    feh,feh_idx=find_nearest(fehgrid,feh)\n",
    "    age,age_idx=find_nearest(logagegrid,age)\n",
    "\n",
    "    return feh_idx*len(logagegrid)+age_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_index(name):\n",
    "    if name in x_columns:\n",
    "        return np.where(np.array(x_columns)==name)[0][0]\n",
    "    else:\n",
    "        return np.where(np.array(isoc_columns)==name)[0][0] +len(x_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['logg', 'logteff', 'logl', 'mass', 'logage', 'feh', 'phase',\n",
       "       'Gaia_RP_EDR3', 'Gaia_BP_EDR3', 'Gaia_G_EDR3', 'BPRP', 'p_slopes',\n",
       "       'slopes', 'low_c', 'high_c'], dtype='<U12')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isoc_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data we have loaded in is as follows:\n",
    "\n",
    "1. x_values is a Numpy array of size (n_samples,n_features,longest_isochrone_tang_length). Each sample has n_features which are copied into the 3rd axis the same number of times as the longest isochrone is.\n",
    "2. Stacked_isochrones is a Numpy array of size (n_isochrones,n_features,largest_tangent_numb_size). Each isochrone will have a certain number of slopes and p_slopes depending on the isochrone. These values extend out into the third axis, however they are padded with NaN values.\n",
    "3. x_input and err are easier access versions, used for input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAESTAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'#torch.device(\"mps\")\n",
    "torch.backends.mps.is_available()\n",
    "\n",
    "sample_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input=torch.tensor(x_input.values[:,1:6],requires_grad=True)\n",
    "x_input=x_input.reshape((x_input.shape)+(1,))\n",
    "x_input_err=torch.tensor(x_input_err.values[:,1:],requires_grad=True)\n",
    "x_input_err=x_input_err.reshape((x_input_err.shape)+(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max=torch.max(x_input,0)[0]\n",
    "x_min=x_input.min(axis=0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input=(x_input-x_min)/(x_max-x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input_err=x_input_err/((x_max-x_min)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input=x_input.repeat(1,1,sample_size)\n",
    "x_input_err=x_input_err.repeat(1,1,sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation over\n",
    "1. x_input is the input for the encoder and x_input_err is the error input\n",
    "2. x_values are the inputs for the decoder and stacked_isochrones are too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VaeStar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class encoder(nn.Module): #q(z|x)\n",
    "    def __init__(self,input_dim,hidden_dims,z_dim):\n",
    "        super().__init__()\n",
    "        # Shapes\n",
    "        self.sample_size=32\n",
    "        self.input_dim=1\n",
    "        self.n_layers=2\n",
    "        self.lstm_hidden_dim=5\n",
    "\n",
    "        self.z_dim=z_dim\n",
    "\n",
    "        self.MV_N=torch.distributions.MultivariateNormal(torch.tensor([0.0 for i in range(self.sample_size)]),torch.eye(self.sample_size))\n",
    "\n",
    "        # Model Definition\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.tanh=nn.Tanh()\n",
    "        #the shape will be (batch_size,sequencelength=1,input_dim=1)\n",
    "        self.dist_lstm=nn.LSTM(self.input_dim,self.lstm_hidden_dim,self.n_layers,batch_first=True)\n",
    "        self.lstm_dense=nn.Linear(in_features=self.sample_size*self.lstm_hidden_dim,out_features=hidden_dims[1])\n",
    "        self.lstm_activation=nn.Sigmoid()\n",
    "\n",
    "        self.input_dense=nn.Linear(in_features=input_dim,out_features=hidden_dims[0])\n",
    "        self.hidden_dense=nn.Linear(in_features=hidden_dims[0],out_features=hidden_dims[1])\n",
    "        self.input_activation=nn.Sigmoid()\n",
    "        self.hidden_activation=nn.Sigmoid()\n",
    "\n",
    "        self.concat_dense=nn.Linear(in_features=hidden_dims[1]*2,out_features=z_dim*2)\n",
    "        self.z_activation=nn.ReLU() #this will mean that extinction cant be negative (this is actually a part of the prior i suppose), could also just do linear\n",
    "\n",
    "        self.N=torch.distributions.Normal(0,1) #prior on extinction\n",
    "\n",
    "        \n",
    "    def forward(self, x,x_err):\n",
    "        # adjust the data\n",
    "        \n",
    "\n",
    "\n",
    "        x_p=x[:,2,:] #very specific to form of data\n",
    "        x_np=x[:,[0,1,3,4],:]\n",
    "        x_np=x_np.reshape((x_np.shape[0]*x_np.shape[2],x_np.shape[1])) #stacking and will average later\n",
    "        #x_np[:,3]=x_np[:,3]+5*torch.log10(x_p.reshape((x_p.shape[0]*x_p.shape[1]))/1000)+5 #absolute magnitude\n",
    "        # absolute magnitude is messing everything up - need to change to make sure there are no nans., that will mean a prior on the distance.\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        #Neural Network - Not parallax\n",
    "        x_np=self.input_activation(self.input_dense(x_np))\n",
    "        x_np=self.hidden_activation(self.hidden_dense(x_np))\n",
    "        print(x_np)\n",
    "\n",
    "        x_np=torch.mean(x_np.reshape((int(x_np.shape[0]/self.sample_size),x_np.shape[1],int(self.sample_size))),-1)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Neural Network LSTM - parallax\n",
    "        h_0 = torch.zeros(2, x_p.size(0), self.lstm_hidden_dim) #hidden state\n",
    "        c_0 = torch.zeros(2, x_p.size(0), self.lstm_hidden_dim) #internal state\n",
    "        # Propagate input through LSTM\n",
    "\n",
    "        x_p, (hn,cn) =self.dist_lstm(x_p.reshape(x_p.shape+(1,)),(h_0,c_0))\n",
    "        \n",
    "        x_p=self.lstm_activation(self.lstm_dense(x_p.reshape(x_p.shape[0],x_p.shape[1]*x_p.shape[2]))) # is this too much magic\n",
    "        \n",
    "        \n",
    "        #concatenate channež\n",
    "        x=torch.concat([x_np,x_p],axis=1)\n",
    "\n",
    "        output=self.concat_dense(x)\n",
    "\n",
    "        \n",
    "        #sample a z value now\n",
    "        z_mu=output[:,:self.z_dim]\n",
    "        z_mu[:,:2]=2*self.tanh(z_mu[:,:2])\n",
    "        #z_mu[:,2]=2.25*self.sigmoid(z_mu[:,2])-1.75\n",
    "        z_mu[:,2]=2.65*self.tanh(z_mu[:,2])+7.65\n",
    "        \n",
    "\n",
    "\n",
    "        z_sigma=torch.exp(self.tanh(output[:,self.z_dim:]))\n",
    "        z=z_mu+z_sigma*self.N.sample(z_mu.shape)\n",
    "        # note we have no final activations\n",
    "        \n",
    "        z_sigma=torch.stack(list(map(lambda n: torch.diag(z_sigma[n]),range(len(z_sigma)))))\n",
    "        \n",
    "        q=torch.distributions.multivariate_normal.MultivariateNormal(loc=z_mu,covariance_matrix=z_sigma**2)\n",
    "        p=torch.distributions.multivariate_normal.MultivariateNormal(torch.tensor([0,0,7.5]),torch.diag(torch.tensor([1,1,1.5])))\n",
    "\n",
    "        z_ext=z[:,:2]\n",
    "        #z_feh=z[:,2]\n",
    "        z_age=z[:,2]#z[:,3]\n",
    "        \n",
    "        # variances need to be done\n",
    "        \n",
    "\n",
    "\n",
    "        return z, z_ext,z_age, torch.distributions.kl_divergence(q,p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder,self).__init__()\n",
    "        \n",
    "    def forward(self,z,all_isochrones,x_values):\n",
    "        \n",
    "        z_ext=z[:,:2]\n",
    "        #z_feh=z[:,2]\n",
    "        z_age=z[:,2]#z[:,3]\n",
    "\n",
    "        log_prob=[]\n",
    "        log_l=0.0\n",
    "        \n",
    "        for i in range(len(z)):\n",
    "            isochrone=torch.cat([x_values[i],all_isochrones[isochrone_selector(x_values[i][column_index('mh_gspphot') ][0],z_age[i])]],dim=0)\n",
    "            isochrone=isochrone.reshape((1,)+isochrone.shape)\n",
    "\n",
    " \n",
    "            truth_1=(isochrone[:,column_index('G'),:]+z_ext[i,0]-(isochrone[:,column_index('bp_rp'),:]+z_ext[i,1])*isochrone[:,column_index('p_slopes'),:]<=isochrone[:,column_index('high_c'),:]) #box selection\n",
    "            truth_1=truth_1.reshape(truth_1.shape[0],1,truth_1.shape[1])\n",
    "            truth_2=(isochrone[:,column_index('low_c'),:]<=isochrone[:,column_index('G'),:]+z_ext[i,0]-(isochrone[:,column_index('bp_rp'),:]+z_ext[i,1])*isochrone[:,column_index('p_slopes'),:])\n",
    "            truth_2=truth_2.reshape(truth_2.shape[0],1,truth_2.shape[1])\n",
    "            truth=truth_1*truth_2\n",
    "            # ^box selection\n",
    "\n",
    "            # projection onto the nearest line\n",
    "            x=((1/torch.sqrt(1+isochrone[:,column_index('slopes'),:]**2))*(isochrone[:,column_index('G'),:]+z_ext[i,0]-(isochrone[:,column_index('bp_rp'),:]+z_ext[i,1])*isochrone[:,column_index('slopes'),:]-isochrone[:,column_index('Gaia_G_EDR3'),:] + isochrone[:,column_index('slopes'),:]*isochrone[:,column_index('BPRP'),:]))\n",
    "            # taking the minimum\n",
    "            idx=torch.argmin(torch.abs(x/truth.reshape(x.shape)).nan_to_num(nan=torch.inf),1)\n",
    "            x=x.gather(1,idx.view(-1,1))\n",
    "            #error needs to be corrected for absolute magnitude \n",
    "            x_err=(1/(1+isochrone[:,column_index('slopes'),:]**2))*isochrone[:,column_index('phot_g_mean_mag_error'),:]**2+(isochrone[:,column_index('slopes'),:]*isochrone[:,column_index('bp_rp_error'),:])**2\n",
    "            x_err=x_err.gather(1,idx.view(-1,1))\n",
    "            isochrone=torch.cat((isochrone,x.reshape(x.shape[0],1,x.shape[1]).repeat(1,1,(isochrone).shape[-1]),x_err.reshape(x_err.shape[0],1,x_err.shape[1]).repeat(1,1,(isochrone).shape[-1])),1)\n",
    "   \n",
    "            dist=torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros_like(x),torch.eye(len(x))+torch.diag(x_err**2))\n",
    "            \n",
    "            \n",
    "            \n",
    "            try:# serious issues here\n",
    "                \n",
    "                log_l+=dist.log_prob(x)\n",
    "                log_prob.append(dist.log_prob(x))\n",
    "\n",
    "            except:\n",
    "                \n",
    "                log_l+=0.0\n",
    "                log_prob.append(0.0)\n",
    "\n",
    "        \n",
    "        return log_l,log_prob,z\n",
    "        \n",
    "\n",
    "\n",
    "class VaeStar(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dims,z_dim):\n",
    "        super(VaeStar, self).__init__()\n",
    "        self.encoder=encoder(input_dim,hidden_dims,z_dim)\n",
    "        self.decoder=decoder()\n",
    "    \n",
    "    def forward(self,x_input,x_input_err, x_values, all_isochrones):\n",
    "        \n",
    "        z, z_ext,z_age, kl=self.encoder(x_input,x_input_err)\n",
    "        log_l,log_prob,z=self.decoder(z,all_isochrones,x_values)\n",
    "        \n",
    "        return kl, log_l,log_prob,z\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VaeStar(\n",
       "  (encoder): encoder(\n",
       "    (sigmoid): Sigmoid()\n",
       "    (tanh): Tanh()\n",
       "    (dist_lstm): LSTM(1, 5, num_layers=2, batch_first=True)\n",
       "    (lstm_dense): Linear(in_features=160, out_features=10, bias=True)\n",
       "    (lstm_activation): Sigmoid()\n",
       "    (input_dense): Linear(in_features=4, out_features=10, bias=True)\n",
       "    (hidden_dense): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (input_activation): Sigmoid()\n",
       "    (hidden_activation): Sigmoid()\n",
       "    (concat_dense): Linear(in_features=20, out_features=6, bias=True)\n",
       "    (z_activation): ReLU()\n",
       "  )\n",
       "  (decoder): decoder()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=1e-3\n",
    "model=VaeStar(input_dim=x_input.shape[1]-1,hidden_dims=[10,10],z_dim=3)\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50#draw_size #need to make sure everything adds up\n",
    "lr = 1e-3\n",
    "epochs = 50\n",
    "sample_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([851, 5, 32])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mattocallaghan/VaeStar/vaestar.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000042?line=0'>1</a>\u001b[0m MV_N\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mMultivariateNormal(torch\u001b[39m.\u001b[39mtensor([\u001b[39m0.0\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sample_size)]),torch\u001b[39m.\u001b[39meye(sample_size))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000042?line=2'>3</a>\u001b[0m eps\u001b[39m=\u001b[39mMV_N\u001b[39m.\u001b[39msample()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000042?line=3'>4</a>\u001b[0m x\u001b[39m=\u001b[39mx\u001b[39m+\u001b[39meps\u001b[39m.\u001b[39mto(device)\u001b[39m*\u001b[39mx_err\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "MV_N=torch.distributions.MultivariateNormal(torch.tensor([0.0 for i in range(sample_size)]),torch.eye(sample_size))\n",
    " \n",
    "eps=MV_N.sample()\n",
    "x=x+eps.to(device)*x_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat=torch.cat([x_input,x_input_err],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values=torch.tensor(x_values)\n",
    "stacked_isochrones=torch.tensor(stacked_isochrones)\n",
    "from torch.utils.data import DataLoader\n",
    "x_cat=DataLoader(x_cat.float(),batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4205, 0.4749, 0.4341,  ..., 0.2918, 0.4731, 0.5128],\n",
      "        [0.4216, 0.4606, 0.4312,  ..., 0.2915, 0.4754, 0.5152],\n",
      "        [0.4251, 0.4661, 0.4397,  ..., 0.2890, 0.4730, 0.5079],\n",
      "        ...,\n",
      "        [0.4237, 0.4598, 0.4362,  ..., 0.2883, 0.4755, 0.5117],\n",
      "        [0.4237, 0.4599, 0.4360,  ..., 0.2884, 0.4755, 0.5118],\n",
      "        [0.4237, 0.4598, 0.4361,  ..., 0.2883, 0.4755, 0.5117]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (50, 3)) of distribution MultivariateNormal(loc: torch.Size([50, 3]), covariance_matrix: torch.Size([50, 3, 3])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan]], grad_fn=<ExpandBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/mattocallaghan/VaeStar/vaestar.ipynb Cell 31'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000033?line=8'>9</a>\u001b[0m x_err\u001b[39m=\u001b[39mx_err\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000033?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000033?line=14'>15</a>\u001b[0m kl, log_l, log_prob, z \u001b[39m=\u001b[39m model(x,x_err,x_values,stacked_isochrones)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000033?line=16'>17</a>\u001b[0m loss\u001b[39m=\u001b[39mkl\u001b[39m.\u001b[39mmean()\u001b[39m-\u001b[39mlog_l\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000033?line=17'>18</a>\u001b[0m overall_loss\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py:1357\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1351'>1352</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1352'>1353</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1353'>1354</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1354'>1355</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1355'>1356</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1356'>1357</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1357'>1358</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1358'>1359</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/mattocallaghan/VaeStar/vaestar.ipynb Cell 24'\u001b[0m in \u001b[0;36mVaeStar.forward\u001b[0;34m(self, x_input, x_input_err, x_values, all_isochrones)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000024?line=60'>61</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x_input,x_input_err, x_values, all_isochrones):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000024?line=62'>63</a>\u001b[0m     z, z_ext,z_age, kl\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x_input,x_input_err)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000024?line=63'>64</a>\u001b[0m     log_l,log_prob,z\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(z,all_isochrones,x_values)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000024?line=65'>66</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m kl, log_l,log_prob,z\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py:1357\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1351'>1352</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1352'>1353</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1353'>1354</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1354'>1355</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1355'>1356</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1356'>1357</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1357'>1358</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1358'>1359</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/mattocallaghan/VaeStar/vaestar.ipynb Cell 23'\u001b[0m in \u001b[0;36mencoder.forward\u001b[0;34m(self, x, x_err)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000023?line=82'>83</a>\u001b[0m \u001b[39m# note we have no final activations\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000023?line=84'>85</a>\u001b[0m z_sigma\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mstack(\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m n: torch\u001b[39m.\u001b[39mdiag(z_sigma[n]),\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(z_sigma)))))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000023?line=86'>87</a>\u001b[0m q\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39;49mdistributions\u001b[39m.\u001b[39;49mmultivariate_normal\u001b[39m.\u001b[39;49mMultivariateNormal(loc\u001b[39m=\u001b[39;49mz_mu,covariance_matrix\u001b[39m=\u001b[39;49mz_sigma\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000023?line=87'>88</a>\u001b[0m p\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mmultivariate_normal\u001b[39m.\u001b[39mMultivariateNormal(torch\u001b[39m.\u001b[39mtensor([\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m7.5\u001b[39m]),torch\u001b[39m.\u001b[39mdiag(torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1.5\u001b[39m])))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000023?line=89'>90</a>\u001b[0m z_ext\u001b[39m=\u001b[39mz[:,:\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py:150\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py?line=146'>147</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc \u001b[39m=\u001b[39m loc\u001b[39m.\u001b[39mexpand(batch_shape \u001b[39m+\u001b[39m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,))\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py?line=148'>149</a>\u001b[0m event_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py?line=149'>150</a>\u001b[0m \u001b[39msuper\u001b[39;49m(MultivariateNormal, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, event_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py?line=151'>152</a>\u001b[0m \u001b[39mif\u001b[39;00m scale_tril \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py?line=152'>153</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unbroadcasted_scale_tril \u001b[39m=\u001b[39m scale_tril\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py:56\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=53'>54</a>\u001b[0m         valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=54'>55</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=55'>56</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=56'>57</a>\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=57'>58</a>\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=58'>59</a>\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=59'>60</a>\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=60'>61</a>\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=61'>62</a>\u001b[0m             )\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=62'>63</a>\u001b[0m \u001b[39msuper\u001b[39m(Distribution, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (50, 3)) of distribution MultivariateNormal(loc: torch.Size([50, 3]), covariance_matrix: torch.Size([50, 3, 3])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan]], grad_fn=<ExpandBackward0>)"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    overall_loss=0.0\n",
    "    for batch_idx,x in enumerate(x_cat):\n",
    "        x,x_err=torch.split(x,split_size_or_sections=int((x.shape[1]/2)),dim=1)\n",
    "        x=x.view(batch_size,x.shape[1],x.shape[2])\n",
    "        x_err=x_err.view(batch_size,x_err.shape[1],x_err.shape[2])\n",
    "        x=x.to(device)\n",
    "        x_err=x_err.to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        kl, log_l, log_prob, z = model(x,x_err,x_values,stacked_isochrones)\n",
    "\n",
    "        loss=kl.mean()-log_l\n",
    "        overall_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss / ((batch_idx+1)*batch_size))\n",
    "    print(\"Overall Loss: \", overall_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9616, -2.2709, -2.1299, -3.6078, -1.6145,  0.1445,  1.0100, -0.4319,\n",
       "        -0.9180, -1.0836, -0.9469, -2.0988, -0.2342, -1.9720, -1.5815, -3.4240,\n",
       "        -2.7625, -0.8353, -2.4097, -0.5739, -1.2914, -1.4979, -2.2890, -3.5471,\n",
       "        -1.8806,  0.2739, -1.8944, -1.1437, -2.3654, -2.1998, -0.9713, -0.3595,\n",
       "        -2.7996, -1.5337, -0.3985,  0.1328, -1.2754, -3.1301,  0.3384, -1.5372,\n",
       "        -3.9976, -3.0134, -2.4749,  0.2650, -2.5529, -1.4431, -0.5315, -5.2530,\n",
       "        -3.2311, -1.8640, -0.0294, -3.0989, -1.2733, -1.5375, -3.5301, -1.4240,\n",
       "        -0.7318,  0.4610, -3.2573, -1.9152, -2.3645, -0.0736, -0.8121, -2.0180,\n",
       "        -1.3153, -1.8200, -1.8575, -1.7040, -1.8140, -3.0763, -1.9474,  0.8322,\n",
       "        -3.3599, -2.1923, -3.0234, -4.1302, -0.8596, -2.3136, -1.8117, -2.5437,\n",
       "        -3.5837, -0.6596, -1.0971, -1.0426, -2.1915, -2.2607, -2.2000, -2.1488,\n",
       "        -2.7520, -2.3063,  0.8070, -2.8988, -0.1996, -1.8477, -2.6091, -1.1129,\n",
       "         0.3856, -2.4116, -2.7115, -2.1628], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this makes no sense it has no way of knowing the x in the output space need to check the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('astro')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "388258d5dbd248a427ffb3173d4e076e3a6ff0d999334480b98d684a9efba49c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
