{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Data')\n",
    "pkl_file = open('isochrones.pkl', 'rb')\n",
    "stacked_isochrones = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('columns.pkl', 'rb')\n",
    "x_columns = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('x_values.pkl', 'rb')\n",
    "x_values = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('isoc_cols.pkl', 'rb')\n",
    "isoc_columns = pickle.load(pkl_file)\n",
    "\n",
    "x_input=pd.read_csv('x_input')\n",
    "x_input_err=pd.read_csv('x_input_err')\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.60605376,   3.88205228,   1.97221484, ...,          nan,\n",
       "                 nan,          nan],\n",
       "       [  3.12640321,   0.69069069,   5.        , ...,          nan,\n",
       "                 nan,          nan],\n",
       "       [  5.        ,  -4.        ,   0.        , ...,          nan,\n",
       "                 nan,          nan],\n",
       "       ...,\n",
       "       [-18.62651499,  -0.09983469,  -0.03972292, ...,          nan,\n",
       "                 nan,          nan],\n",
       "       [ -0.04062704,  24.6141491 , -20.90347589, ...,          nan,\n",
       "                 nan,          nan],\n",
       "       [-22.83590309, -22.51460167,   1.46087523, ...,          nan,\n",
       "                 nan,          nan]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_isochrones[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    #array = np.asarray(array)\n",
    "    idx = (torch.abs(array - value)).argmin()\n",
    "    return array[idx],idx\n",
    "\n",
    "def isochrone_selector(feh,age):\n",
    "    '''if(feh<-4 or feh>0.5):\n",
    "        raise NotImplementedError\n",
    "    if(age<5 or age>10.3):\n",
    "        raise NotImplementedError\n",
    "    else:'''\n",
    "    logagegrid = torch.tensor(np.linspace(5,10.3,105))\n",
    "    fehgrid = torch.tensor(np.linspace(-4,0.5,90))\n",
    "    feh,feh_idx=find_nearest(fehgrid,feh)\n",
    "    age,age_idx=find_nearest(logagegrid,age)\n",
    "\n",
    "    return feh_idx*len(logagegrid)+age_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_index(name):\n",
    "    if name in x_columns:\n",
    "        return np.where(np.array(x_columns)==name)[0][0]\n",
    "    else:\n",
    "        return np.where(np.array(isoc_columns)==name)[0][0] +len(x_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['logg', 'logteff', 'logl', 'mass', 'logage', 'feh', 'phase',\n",
       "       'Gaia_RP_EDR3', 'Gaia_BP_EDR3', 'Gaia_G_EDR3', 'BPRP', 'p_slopes',\n",
       "       'slopes', 'low_c', 'high_c'], dtype='<U12')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isoc_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data we have loaded in is as follows:\n",
    "\n",
    "1. x_values is a Numpy array of size (n_samples,n_features,longest_isochrone_tang_length). Each sample has n_features which are copied into the 3rd axis the same number of times as the longest isochrone is.\n",
    "2. Stacked_isochrones is a Numpy array of size (n_isochrones,n_features,largest_tangent_numb_size). Each isochrone will have a certain number of slopes and p_slopes depending on the isochrone. These values extend out into the third axis, however they are padded with NaN values.\n",
    "3. x_input and err are easier access versions, used for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.73925433e+01, -1.75523825e+01, -1.74907631e+01, -1.59839254e-01,\n",
       "        7.07219486e-02, -1.41398819e+01, -1.76695745e+01, -1.74794589e+01,\n",
       "        1.99925974e+00,  4.14005443e+00,  9.40709167e+00,  1.93193193e+00,\n",
       "        5.00000000e+00, -4.00000000e+00,  0.00000000e+00, -1.75908508e+01,\n",
       "       -1.77373116e+01, -1.76799325e+01, -1.46460821e-01,  6.14792768e-02,\n",
       "       -1.62656435e+01, -1.78577022e+01, -1.76709282e+01,  1.97464221e+00,\n",
       "        4.12802174e+00,  9.45416162e+00,  1.95195195e+00,  5.00000000e+00,\n",
       "       -4.00000000e+00,  0.00000000e+00, -1.77842444e+01, -1.79192657e+01,\n",
       "       -1.78660032e+01, -1.35021331e-01,  6.15102157e-02, -1.62574621e+01,\n",
       "       -1.80437757e+01, -1.78576980e+01,  1.95036799e+00,  4.11602791e+00,\n",
       "        9.50104221e+00,  1.97197197e+00,  5.00000000e+00, -4.00000000e+00,\n",
       "        0.00000000e+00, -1.79769199e+01, -1.81005387e+01, -1.80513795e+01,\n",
       "       -1.23618792e-01,  5.92113304e-02, -1.68886595e+01, -1.82252834e+01,\n",
       "       -1.80440599e+01,  1.92636169e+00,  4.10404728e+00,  9.54770668e+00,\n",
       "        1.99199199e+00,  5.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "       -1.81643552e+01, -1.82772810e+01, -1.82319699e+01, -1.12925800e-01,\n",
       "        5.48139301e-02, -1.82435377e+01, -1.84028985e+01, -1.82257800e+01,\n",
       "        1.90546462e+00,  4.09305123e+00,  9.59519424e+00,  2.01201201e+00,\n",
       "        5.00000000e+00, -4.00000000e+00,  0.00000000e+00, -1.83471393e+01,\n",
       "       -1.84503856e+01, -1.84085578e+01, -1.03246318e-01,  1.56527625e-02,\n",
       "       -6.38864864e+01, -1.86249523e+01, -1.84069418e+01,  1.88145118e+00,\n",
       "        4.08917556e+00,  9.67421849e+00,  2.03203203e+00,  5.00000000e+00,\n",
       "       -4.00000000e+00,  0.00000000e+00, -1.85672803e+01, -1.86671150e+01,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_isochrones[0][column_index('p_slopes')-len(x_columns)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAESTAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'#torch.device(\"mps\")\n",
    "torch.backends.mps.is_available()\n",
    "\n",
    "sample_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input=torch.tensor(x_input.values[:,1:6],requires_grad=True)\n",
    "x_input=x_input.reshape((x_input.shape)+(1,))\n",
    "x_input_err=torch.tensor(x_input_err.values[:,1:],requires_grad=True)\n",
    "x_input_err=x_input_err.reshape((x_input_err.shape)+(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max=torch.max(x_input,0)[0]\n",
    "x_min=x_input.min(axis=0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input=(x_input-x_min)/(x_max-x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input_err=x_input_err/(x_max-x_min)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input=x_input.repeat(1,1,sample_size)\n",
    "x_input_err=x_input_err.repeat(1,1,sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation over\n",
    "1. x_input is the input for the encoder and x_input_err is the error input\n",
    "2. x_values are the inputs for the decoder and stacked_isochrones are too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VaeStar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class encoder(nn.Module): #q(z|x)\n",
    "    def __init__(self,input_dim,hidden_dims,z_dim):\n",
    "        super().__init__()\n",
    "        # Shapes\n",
    "        self.sample_size=32\n",
    "        self.input_dim=1\n",
    "        self.n_layers=2\n",
    "        self.lstm_hidden_dim=5\n",
    "\n",
    "        self.z_dim=z_dim\n",
    "\n",
    "        self.MV_N=torch.distributions.MultivariateNormal(torch.tensor([0.0 for i in range(self.sample_size)]),torch.eye(self.sample_size))\n",
    "\n",
    "        # Model Definition\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        #the shape will be (batch_size,sequencelength=1,input_dim=1)\n",
    "        self.dist_lstm=nn.LSTM(self.input_dim,self.lstm_hidden_dim,self.n_layers,batch_first=True)\n",
    "        self.lstm_dense=nn.Linear(in_features=self.sample_size*self.lstm_hidden_dim,out_features=hidden_dims[1])\n",
    "        self.lstm_activation=nn.Tanh()\n",
    "\n",
    "        self.input_dense=nn.Linear(in_features=input_dim,out_features=hidden_dims[0])\n",
    "        self.hidden_dense=nn.Linear(in_features=hidden_dims[0],out_features=hidden_dims[1])\n",
    "        self.input_activation=nn.ReLU()\n",
    "        self.hidden_activation=nn.ReLU()\n",
    "\n",
    "        self.concat_dense=nn.Linear(in_features=hidden_dims[1]*2,out_features=z_dim*2)\n",
    "        self.z_activation=nn.ReLU() #this will mean that extinction cant be negative (this is actually a part of the prior i suppose), could also just do linear\n",
    "\n",
    "        self.N=torch.distributions.Normal(0,1) #prior on extinction\n",
    "\n",
    "        \n",
    "    def forward(self, x,x_err):\n",
    "        # adjust the data\n",
    "        \n",
    "        eps=self.MV_N.sample()\n",
    "        x=x+eps.to(device)*x_err\n",
    "        \n",
    "        x_p=x[:,2,:] #very specific to form of data\n",
    "        x_np=x[:,[0,1,3,4],:]\n",
    "        x_np=x_np.reshape((x_np.shape[0]*x_np.shape[2],x_np.shape[1])) #stacking and will average later\n",
    "        #x_np[:,3]=x_np[:,3]+5*torch.log10(x_p.reshape((x_p.shape[0]*x_p.shape[1]))/1000)+5 #absolute magnitude\n",
    "        # absolute magnitude is messing everything up - need to change to make sure there are no nans., that will mean a prior on the distance.\n",
    "        \n",
    "        #Neural Network - Not parallax\n",
    "        x_np=self.input_activation(self.input_dense(x_np))\n",
    "        x_np=self.hidden_activation(self.hidden_dense(x_np))\n",
    "        x_np=torch.mean(x_np.reshape((int(x_np.shape[0]/self.sample_size),x_np.shape[1],int(self.sample_size))),-1)\n",
    "\n",
    "\n",
    "        # Neural Network LSTM - parallax\n",
    "        h_0 = torch.zeros(2, x_p.size(0), self.lstm_hidden_dim) #hidden state\n",
    "        c_0 = torch.zeros(2, x_p.size(0), self.lstm_hidden_dim) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        \n",
    "        x_p, (hn,cn) =self.dist_lstm(x_p.reshape(x_p.shape+(1,)),(h_0,c_0))\n",
    "        x_p=self.lstm_activation(self.lstm_dense(x_p.reshape(x_p.shape[0],x_p.shape[1]*x_p.shape[2]))) # is this too much magic\n",
    "        \n",
    "        #concatenate channež\n",
    " \n",
    "        x=torch.concat([x_np,x_p],axis=1)\n",
    "        \n",
    "        output=self.concat_dense(x)\n",
    "\n",
    "\n",
    "        #sample a z value now\n",
    "        z_mu=output[:,:self.z_dim]\n",
    "        z_mu[:,:2]=2*self.sigmoid(z_mu[:,:2])\n",
    "        z_mu[:,2]=2.25*self.sigmoid(z_mu[:,2])-1.75\n",
    "        z_mu[:,3]=2.65*self.sigmoid(z_mu[:,2])+7.65\n",
    "\n",
    "\n",
    "        z_sigma=torch.exp(self.sigmoid(output[:,self.z_dim:]))\n",
    "        z=z_mu+z_sigma*self.N.sample(z_mu.shape)\n",
    "        # note we have no final activations\n",
    "        \n",
    "        z_sigma=torch.stack(list(map(lambda n: torch.diag(z_sigma[n]),range(len(z_sigma)))))\n",
    "        q=torch.distributions.multivariate_normal.MultivariateNormal(loc=z_mu,covariance_matrix=z_sigma**2)\n",
    "        p=torch.distributions.multivariate_normal.MultivariateNormal(torch.tensor([0,0,-2,7.5]),torch.diag(torch.tensor([1,1,1.5,1.5])))\n",
    "\n",
    "        z_ext=z[:,:2]\n",
    "        z_feh=z[:,2]\n",
    "        z_age=z[:,3]\n",
    "        \n",
    "        # variances need to be done\n",
    "        \n",
    "        \n",
    "\n",
    "        return z, z_ext,z_feh,z_age, torch.distributions.kl_divergence(q,p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder,self).__init__()\n",
    "        \n",
    "    def forward(self,z,all_isochrones,x_values):\n",
    "        \n",
    "        z_ext=z[:,:2]\n",
    "        z_feh=z[:,2]\n",
    "        z_age=z[:,3]\n",
    "\n",
    "        log_prob=[]\n",
    "        log_l=0.0\n",
    "        \n",
    "        for i in range(len(z)):\n",
    "\n",
    "            isochrone=torch.cat([x_values[i],all_isochrones[isochrone_selector(z_feh[i],z_age[i])]],dim=0)\n",
    "\n",
    "            isochrone=isochrone.reshape((1,)+isochrone.shape)\n",
    "            print(z_ext.shape)\n",
    "            print(isochrone[:,column_index('G'),:].shape)\n",
    "            truth_1=(isochrone[:,column_index('G'),:]+z_ext[i,0]-(isochrone[:,column_index('bp_rp'),:]+z_ext[i,1])*isochrone[:,column_index('p_slopes'),:]<=isochrone[:,column_index('high_c'),:]) #box selection\n",
    "            truth_1=truth_1.reshape(truth_1.shape[0],1,truth_1.shape[1])\n",
    "            truth_2=(isochrone[:,column_index('low_c'),:]<=isochrone[:,column_index('G'),:]+z_ext[i,0]-(isochrone[:,column_index('bp_rp'),:]+z_ext[i,1])*isochrone[:,column_index('p_slopes'),:])\n",
    "            truth_2=truth_2.reshape(truth_2.shape[0],1,truth_2.shape[1])\n",
    "            truth=truth_1*truth_2\n",
    "            # ^box selection\n",
    "\n",
    "            # projection onto the nearest line\n",
    "            x=((1/torch.sqrt(1+isochrone[:,column_index('slopes'),:]**2))*(isochrone[:,column_index('G'),:]+z_ext[i,0]-(isochrone[:,column_index('bp_rp'),:]+z_ext[i,1])*isochrone[:,column_index('slopes'),:]-isochrone[:,column_index('Gaia_G_EDR3'),:] + isochrone[:,column_index('slopes'),:]*isochrone[:,column_index('BPRP'),:]))\n",
    "            # taking the minimum\n",
    "            idx=torch.argmin(torch.abs(x/truth.reshape(x.shape)).nan_to_num(nan=torch.inf),1)\n",
    "            x=x.gather(1,idx.view(-1,1))\n",
    "            #error needs to be corrected for absolute magnitude \n",
    "            x_err=(1/(1+isochrone[:,column_index('slopes'),:]**2))*isochrone[:,column_index('phot_g_mean_mag_error'),:]**2+(isochrone[:,column_index('slopes'),:]*isochrone[:,column_index('bp_rp_error'),:])**2\n",
    "            x_err=x_err.gather(1,idx.view(-1,1))\n",
    "            isochrone=torch.cat((isochrone,x.reshape(x.shape[0],1,x.shape[1]).repeat(1,1,(isochrone).shape[-1]),x_err.reshape(x_err.shape[0],1,x_err.shape[1]).repeat(1,1,(isochrone).shape[-1])),1)\n",
    "                            \n",
    "            dist=torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros_like(x),torch.eye(len(x))+torch.diag(x_err**2))\n",
    "            \n",
    "            \n",
    "            \n",
    "            try:# serious issues here\n",
    "                \n",
    "                log_l+=dist.log_prob(x)\n",
    "                log_prob.append(dist.log_prob(x))\n",
    "\n",
    "            except:\n",
    "                \n",
    "                log_l+=0.0\n",
    "                log_prob.append(0.0)\n",
    "\n",
    "        \n",
    "        return log_l,log_prob,z\n",
    "        \n",
    "\n",
    "\n",
    "class VaeStar(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dims,z_dim):\n",
    "        super(VaeStar, self).__init__()\n",
    "        self.encoder=encoder(input_dim,hidden_dims,z_dim)\n",
    "        self.decoder=decoder()\n",
    "    \n",
    "    def forward(self,x_input,x_input_err, x_values, all_isochrones):\n",
    "        \n",
    "        z, z_ext,z_feh,z_age, kl=self.encoder(x_input,x_input_err)\n",
    "        log_l,log_prob,z=self.decoder(z,all_isochrones,x_values)\n",
    "        \n",
    "        return kl, log_l,log_prob,z\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VaeStar(\n",
       "  (encoder): encoder(\n",
       "    (sigmoid): Sigmoid()\n",
       "    (dist_lstm): LSTM(1, 5, num_layers=2, batch_first=True)\n",
       "    (lstm_dense): Linear(in_features=160, out_features=10, bias=True)\n",
       "    (lstm_activation): Tanh()\n",
       "    (input_dense): Linear(in_features=4, out_features=10, bias=True)\n",
       "    (hidden_dense): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (input_activation): ReLU()\n",
       "    (hidden_activation): ReLU()\n",
       "    (concat_dense): Linear(in_features=20, out_features=8, bias=True)\n",
       "    (z_activation): ReLU()\n",
       "  )\n",
       "  (decoder): decoder()\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=1e-3\n",
    "model=VaeStar(input_dim=x_input.shape[1]-1,hidden_dims=[10,10],z_dim=4)\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100#draw_size #need to make sure everything adds up\n",
    "lr = 1e-3\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat=torch.cat([x_input,x_input_err],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values=torch.tensor(x_values)\n",
    "stacked_isochrones=torch.tensor(stacked_isochrones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "x_cat=DataLoader(x_cat.float(),batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.8261e+02,  2.8261e+02,  2.8261e+02,  ...,  2.8261e+02,\n",
       "          2.8261e+02,  2.8261e+02],\n",
       "        [-6.2860e+00, -6.2860e+00, -6.2860e+00,  ..., -6.2860e+00,\n",
       "         -6.2860e+00, -6.2860e+00],\n",
       "        [ 5.0739e-01,  5.0739e-01,  5.0739e-01,  ...,  5.0739e-01,\n",
       "          5.0739e-01,  5.0739e-01],\n",
       "        ...,\n",
       "        [ 4.9026e-03,  4.9026e-03,  4.9026e-03,  ...,  4.9026e-03,\n",
       "          4.9026e-03,  4.9026e-03],\n",
       "        [ 7.6935e-03,  7.6935e-03,  7.6935e-03,  ...,  7.6935e-03,\n",
       "          7.6935e-03,  7.6935e-03],\n",
       "        [ 4.9308e+00,  4.9308e+00,  4.9308e+00,  ...,  4.9308e+00,\n",
       "          4.9308e+00,  4.9308e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([1, 457])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (100, 4)) of distribution MultivariateNormal(loc: torch.Size([100, 4]), covariance_matrix: torch.Size([100, 4, 4])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan]], grad_fn=<ExpandBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tm/gnjj0w_d6vj9x6q_ww3f6jsw0000gn/T/ipykernel_29944/1525012411.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mkl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_err\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstacked_isochrones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlog_l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1355\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/tm/gnjj0w_d6vj9x6q_ww3f6jsw0000gn/T/ipykernel_29944/2530011566.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_input, x_input_err, x_values, all_isochrones)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_input_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_isochrones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_ext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_feh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_age\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_input_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mlog_l\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_isochrones\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1355\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/tm/gnjj0w_d6vj9x6q_ww3f6jsw0000gn/T/ipykernel_29944/3173538263.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, x_err)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mz_sigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_sigma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_sigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz_mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz_sigma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/astro/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mevent_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscale_tril\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m     57\u001b[0m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (100, 4)) of distribution MultivariateNormal(loc: torch.Size([100, 4]), covariance_matrix: torch.Size([100, 4, 4])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan],\n        [nan, nan, nan, nan]], grad_fn=<ExpandBackward0>)"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    overall_loss=0.0\n",
    "    for batch_idx,x in enumerate(x_cat):\n",
    "        x,x_err=torch.split(x,split_size_or_sections=int((x.shape[1]/2)),dim=1)\n",
    "        x=x.view(batch_size,x.shape[1],x.shape[2])\n",
    "        x_err=x_err.view(batch_size,x_err.shape[1],x_err.shape[2])\n",
    "        x=x.to(device)\n",
    "        x_err=x_err.to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        kl, log_l, log_prob, z = model(x,x_err,x_values,stacked_isochrones)\n",
    "\n",
    "        loss=kl.mean()-log_l\n",
    "        overall_loss+=loss.item()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss / ((batch_idx+1)*batch_size))\n",
    "    print(\"Overall Loss: \", overall_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9616, -2.2709, -2.1299, -3.6078, -1.6145,  0.1445,  1.0100, -0.4319,\n",
       "        -0.9180, -1.0836, -0.9469, -2.0988, -0.2342, -1.9720, -1.5815, -3.4240,\n",
       "        -2.7625, -0.8353, -2.4097, -0.5739, -1.2914, -1.4979, -2.2890, -3.5471,\n",
       "        -1.8806,  0.2739, -1.8944, -1.1437, -2.3654, -2.1998, -0.9713, -0.3595,\n",
       "        -2.7996, -1.5337, -0.3985,  0.1328, -1.2754, -3.1301,  0.3384, -1.5372,\n",
       "        -3.9976, -3.0134, -2.4749,  0.2650, -2.5529, -1.4431, -0.5315, -5.2530,\n",
       "        -3.2311, -1.8640, -0.0294, -3.0989, -1.2733, -1.5375, -3.5301, -1.4240,\n",
       "        -0.7318,  0.4610, -3.2573, -1.9152, -2.3645, -0.0736, -0.8121, -2.0180,\n",
       "        -1.3153, -1.8200, -1.8575, -1.7040, -1.8140, -3.0763, -1.9474,  0.8322,\n",
       "        -3.3599, -2.1923, -3.0234, -4.1302, -0.8596, -2.3136, -1.8117, -2.5437,\n",
       "        -3.5837, -0.6596, -1.0971, -1.0426, -2.1915, -2.2607, -2.2000, -2.1488,\n",
       "        -2.7520, -2.3063,  0.8070, -2.8988, -0.1996, -1.8477, -2.6091, -1.1129,\n",
       "         0.3856, -2.4116, -2.7115, -2.1628], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this makes no sense it has no way of knowing the x in the output space need to check the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('astro')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "388258d5dbd248a427ffb3173d4e076e3a6ff0d999334480b98d684a9efba49c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
