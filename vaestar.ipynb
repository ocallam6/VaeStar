{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Data')\n",
    "pkl_file = open('isochrones.pkl', 'rb')\n",
    "stacked_isochrones = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('columns.pkl', 'rb')\n",
    "x_columns = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('x_values.pkl', 'rb')\n",
    "x_values = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('isoc_cols.pkl', 'rb')\n",
    "isoc_columns = pickle.load(pkl_file)\n",
    "\n",
    "x_input=pd.read_csv('x_input')\n",
    "x_input_err=pd.read_csv('x_input_err')\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    #array = np.asarray(array)\n",
    "    idx = (torch.abs(array - value)).argmin()\n",
    "    return array[idx],idx\n",
    "\n",
    "def isochrone_selector(feh,age):\n",
    "    '''if(feh<-4 or feh>0.5):\n",
    "        raise NotImplementedError\n",
    "    if(age<5 or age>10.3):\n",
    "        raise NotImplementedError\n",
    "    else:'''\n",
    "    logagegrid = torch.tensor(np.linspace(5,10.3,105))\n",
    "    fehgrid = torch.tensor(np.linspace(-4,0.5,90))\n",
    "    feh,feh_idx=find_nearest(fehgrid,feh)\n",
    "    age,age_idx=find_nearest(logagegrid,age)\n",
    "\n",
    "    return feh_idx*len(logagegrid)+age_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_index(name):\n",
    "    if name in x_columns:\n",
    "        return np.where(np.array(x_columns)==name)[0][0]\n",
    "    else:\n",
    "        return np.where(np.array(isoc_columns)==name)[0][0] +len(x_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141, 282.60587141, 282.60587141, 282.60587141,\n",
       "       282.60587141])"
      ]
     },
     "execution_count": 868,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['logg', 'logteff', 'logl', 'mass', 'logage', 'feh', 'phase',\n",
       "       'Gaia_RP_EDR3', 'Gaia_BP_EDR3', 'Gaia_G_EDR3', 'BPRP', 'p_slopes',\n",
       "       'slopes', 'low_c', 'high_c'], dtype='<U12')"
      ]
     },
     "execution_count": 869,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isoc_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data we have loaded in is as follows:\n",
    "\n",
    "1. x_values is a Numpy array of size (n_samples,n_features,longest_isochrone_tang_length). Each sample has n_features which are copied into the 3rd axis the same number of times as the longest isochrone is.\n",
    "2. Stacked_isochrones is a Numpy array of size (n_isochrones,n_features,largest_tangent_numb_size). Each isochrone will have a certain number of slopes and p_slopes depending on the isochrone. These values extend out into the third axis, however they are padded with NaN values.\n",
    "3. x_input and err are easier access versions, used for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.73925433e+01, -1.75523825e+01, -1.74907631e+01, -1.59839254e-01,\n",
       "        7.07219486e-02, -1.41398819e+01, -1.76695745e+01, -1.74794589e+01,\n",
       "        1.99925974e+00,  4.14005443e+00,  9.40709167e+00,  1.93193193e+00,\n",
       "        5.00000000e+00, -4.00000000e+00,  0.00000000e+00, -1.75908508e+01,\n",
       "       -1.77373116e+01, -1.76799325e+01, -1.46460821e-01,  6.14792768e-02,\n",
       "       -1.62656435e+01, -1.78577022e+01, -1.76709282e+01,  1.97464221e+00,\n",
       "        4.12802174e+00,  9.45416162e+00,  1.95195195e+00,  5.00000000e+00,\n",
       "       -4.00000000e+00,  0.00000000e+00, -1.77842444e+01, -1.79192657e+01,\n",
       "       -1.78660032e+01, -1.35021331e-01,  6.15102157e-02, -1.62574621e+01,\n",
       "       -1.80437757e+01, -1.78576980e+01,  1.95036799e+00,  4.11602791e+00,\n",
       "        9.50104221e+00,  1.97197197e+00,  5.00000000e+00, -4.00000000e+00,\n",
       "        0.00000000e+00, -1.79769199e+01, -1.81005387e+01, -1.80513795e+01,\n",
       "       -1.23618792e-01,  5.92113304e-02, -1.68886595e+01, -1.82252834e+01,\n",
       "       -1.80440599e+01,  1.92636169e+00,  4.10404728e+00,  9.54770668e+00,\n",
       "        1.99199199e+00,  5.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "       -1.81643552e+01, -1.82772810e+01, -1.82319699e+01, -1.12925800e-01,\n",
       "        5.48139301e-02, -1.82435377e+01, -1.84028985e+01, -1.82257800e+01,\n",
       "        1.90546462e+00,  4.09305123e+00,  9.59519424e+00,  2.01201201e+00,\n",
       "        5.00000000e+00, -4.00000000e+00,  0.00000000e+00, -1.83471393e+01,\n",
       "       -1.84503856e+01, -1.84085578e+01, -1.03246318e-01,  1.56527625e-02,\n",
       "       -6.38864864e+01, -1.86249523e+01, -1.84069418e+01,  1.88145118e+00,\n",
       "        4.08917556e+00,  9.67421849e+00,  2.03203203e+00,  5.00000000e+00,\n",
       "       -4.00000000e+00,  0.00000000e+00, -1.85672803e+01, -1.86671150e+01,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan])"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_isochrones[0][column_index('p_slopes')-len(x_columns)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAESTAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'#torch.device(\"mps\")\n",
    "torch.backends.mps.is_available()\n",
    "\n",
    "sample_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input=torch.tensor(x_input.values[:,1:6],requires_grad=True)\n",
    "x_input=x_input.reshape((x_input.shape)+(1,))\n",
    "x_input_err=torch.tensor(x_input_err.values[:,1:],requires_grad=True)\n",
    "x_input_err=x_input_err.reshape((x_input_err.shape)+(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max=torch.max(x_input,0)[0]\n",
    "x_min=x_input.min(axis=0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input=(x_input-x_min)/(x_max-x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input_err=x_input_err/(x_max-x_min)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input=x_input.repeat(1,1,sample_size)\n",
    "x_input_err=x_input_err.repeat(1,1,sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation over\n",
    "1. x_input is the input for the encoder and x_input_err is the error input\n",
    "2. x_values are the inputs for the decoder and stacked_isochrones are too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VaeStar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class encoder(nn.Module): #q(z|x)\n",
    "    def __init__(self,input_dim,hidden_dims,z_dim):\n",
    "        super().__init__()\n",
    "        # Shapes\n",
    "        self.sample_size=32\n",
    "        self.input_dim=1\n",
    "        self.n_layers=2\n",
    "        self.lstm_hidden_dim=5\n",
    "\n",
    "        self.z_dim=z_dim\n",
    "\n",
    "        self.MV_N=torch.distributions.MultivariateNormal(torch.tensor([0.0 for i in range(self.sample_size)]),torch.eye(self.sample_size))\n",
    "\n",
    "        # Model Definition\n",
    "\n",
    "        #the shape will be (batch_size,sequencelength=1,input_dim=1)\n",
    "        self.dist_lstm=nn.LSTM(self.input_dim,self.lstm_hidden_dim,self.n_layers,batch_first=True)\n",
    "        self.lstm_dense=nn.Linear(in_features=self.sample_size*self.lstm_hidden_dim,out_features=hidden_dims[1])\n",
    "        self.lstm_activation=nn.Tanh()\n",
    "\n",
    "        self.input_dense=nn.Linear(in_features=input_dim,out_features=hidden_dims[0])\n",
    "        self.hidden_dense=nn.Linear(in_features=hidden_dims[0],out_features=hidden_dims[1])\n",
    "        self.input_activation=nn.ReLU()\n",
    "        self.hidden_activation=nn.ReLU()\n",
    "\n",
    "        self.concat_dense=nn.Linear(in_features=hidden_dims[1]*2,out_features=z_dim*2)\n",
    "        self.z_activation=nn.ReLU() #this will mean that extinction cant be negative (this is actually a part of the prior i suppose), could also just do linear\n",
    "\n",
    "        self.N=torch.distributions.Normal(0,1) #prior on extinction\n",
    "\n",
    "        \n",
    "    def forward(self, x,x_err):\n",
    "        # adjust the data\n",
    "        \n",
    "        eps=self.MV_N.sample()\n",
    "        x=x+eps.to(device)*x_err\n",
    "        \n",
    "        x_p=x[:,2,:] #very specific to form of data\n",
    "        x_np=x[:,[0,1,3,4],:]\n",
    "        x_np=x_np.reshape((x_np.shape[0]*x_np.shape[2],x_np.shape[1])) #stacking and will average later\n",
    "        #x_np[:,3]=x_np[:,3]+5*torch.log10(x_p.reshape((x_p.shape[0]*x_p.shape[1]))/1000)+5 #absolute magnitude\n",
    "        # absolute magnitude is messing everything up - need to change to make sure there are no nans., that will mean a prior on the distance.\n",
    "        \n",
    "        #Neural Network - Not parallax\n",
    "        x_np=self.input_activation(self.input_dense(x_np))\n",
    "        x_np=self.hidden_activation(self.hidden_dense(x_np))\n",
    "        x_np=torch.mean(x_np.reshape((int(x_np.shape[0]/self.sample_size),x_np.shape[1],int(self.sample_size))),-1)\n",
    "\n",
    "\n",
    "        # Neural Network LSTM - parallax\n",
    "        h_0 = torch.zeros(2, x_p.size(0), self.lstm_hidden_dim) #hidden state\n",
    "        c_0 = torch.zeros(2, x_p.size(0), self.lstm_hidden_dim) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        \n",
    "        x_p, (hn,cn) =self.dist_lstm(x_p.reshape(x_p.shape+(1,)),(h_0,c_0))\n",
    "        x_p=self.lstm_activation(self.lstm_dense(x_p.reshape(x_p.shape[0],x_p.shape[1]*x_p.shape[2]))) # is this too much magic\n",
    "        \n",
    "        #concatenate channež\n",
    "        print(x_np.shape)\n",
    "        print(x_p.shape)\n",
    "        x=torch.concat([x_np,x_p],axis=1)\n",
    "        print(x.shape)\n",
    "        output=self.concat_dense(x)\n",
    "\n",
    "\n",
    "        #sample a z value now\n",
    "        z_mu=output[:,:self.z_dim]\n",
    "        z_sigma=torch.exp(output[:,self.z_dim:])\n",
    "        z=z_mu+z_sigma*self.N.sample(z_mu.shape)\n",
    "        # note we have no final activations\n",
    "        \n",
    "        z_sigma=torch.stack(list(map(lambda n: torch.diag(z_sigma[n]),range(len(z_sigma)))))\n",
    "        q=torch.distributions.multivariate_normal.MultivariateNormal(loc=z_mu,covariance_matrix=z_sigma**2)\n",
    "        p=torch.distributions.multivariate_normal.MultivariateNormal(torch.tensor([0,0,-2,7.5]),torch.diag(torch.tensor([1,1,1.5,1.5])))\n",
    "\n",
    "        z_ext=z[:,:2]\n",
    "        z_feh=z[:,2]\n",
    "        z_age=z[:,3]\n",
    "        \n",
    "        # variances need to be done\n",
    "        \n",
    "        print('flag2')\n",
    "\n",
    "        return z, z_ext,z_feh,z_age, torch.distributions.kl_divergence(q,p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder,self).__init__()\n",
    "        \n",
    "    def forward(self,z,all_isochrones,x_values):\n",
    "        z_ext=z[:,:2]\n",
    "        z_feh=z[:,2]\n",
    "        z_age=z[:,3]\n",
    "\n",
    "        log_prob=[]\n",
    "        log_l=0.0\n",
    "        for i in range(len(z)):\n",
    "            isochrone=torch.cat([x_values[i],all_isochrones[isochrone_selector(z_feh[i],z_age[i])]],dim=0)\n",
    "            isochrone=isochrone.reshape((1,)+isochrone.shape)\n",
    "            truth_1=(isochrone[:,column_index('G'),:]-isochrone[:,column_index('bp_rp'),:]*isochrone[:,column_index('p_slopes'),:]<=isochrone[:,column_index('high_c'),:]) #box selection\n",
    "            truth_1=truth_1.reshape(truth_1.shape[0],1,truth_1.shape[1])\n",
    "            truth_2=(isochrone[:,column_index('low_c'),:]<=isochrone[:,column_index('G'),:]-isochrone[:,column_index('bp_rp'),:]*isochrone[:,column_index('p_slopes'),:])\n",
    "            truth_2=truth_2.reshape(truth_2.shape[0],1,truth_2.shape[1])\n",
    "            truth=truth_1*truth_2\n",
    "            # ^box selection\n",
    "\n",
    "            # projection onto the nearest line\n",
    "            x=((1/torch.sqrt(1+isochrone[:,column_index('slopes'),:]**2))*(isochrone[:,column_index('G'),:]-isochrone[:,column_index('bp_rp'),:]*isochrone[:,column_index('slopes'),:]-isochrone[:,column_index('Gaia_G_EDR3'),:] + isochrone[:,column_index('slopes'),:]*isochrone[:,column_index('BPRP'),:]))\n",
    "            # taking the minimum\n",
    "            idx=torch.argmin(torch.abs(x/truth.reshape(x.shape)).nan_to_num(nan=torch.inf),1)\n",
    "            x=x.gather(1,idx.view(-1,1))\n",
    "            #error needs to be corrected for absolute magnitude \n",
    "            x_err=(1/(1+isochrone[:,column_index('slopes'),:]**2))*isochrone[:,column_index('phot_g_mean_mag_error'),:]**2+(isochrone[:,column_index('slopes'),:]*isochrone[:,column_index('bp_rp_error'),:])**2\n",
    "            x_err=x_err.gather(1,idx.view(-1,1))\n",
    "            isochrone=torch.cat((isochrone,x.reshape(x.shape[0],1,x.shape[1]).repeat(1,1,(isochrone).shape[-1]),x_err.reshape(x_err.shape[0],1,x_err.shape[1]).repeat(1,1,(isochrone).shape[-1])),1)\n",
    "                                \n",
    "            dist=torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros_like(x),torch.eye(len(x))+torch.diag(x_err**2))\n",
    "            log_l+=dist.log_prob(x)\n",
    "            log_prob.append(dist.log_prob(x))\n",
    "        return log_l,log_prob,z\n",
    "        \n",
    "\n",
    "\n",
    "class VaeStar(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dims,z_dim):\n",
    "        super(VaeStar, self).__init__()\n",
    "        self.encoder=encoder(input_dim,hidden_dims,z_dim)\n",
    "        self.decoder=decoder()\n",
    "    \n",
    "    def forward(self,x_input,x_input_err, x_values, all_isochrones):\n",
    "        print('hello')\n",
    "        z, z_ext,z_feh,z_age, kl=self.encoder(x_input,x_input_err)\n",
    "        return kl, self.decoder(z,all_isochrones,x_values)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 881,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VaeStar(\n",
       "  (encoder): encoder(\n",
       "    (dist_lstm): LSTM(1, 5, num_layers=2, batch_first=True)\n",
       "    (lstm_dense): Linear(in_features=160, out_features=10, bias=True)\n",
       "    (lstm_activation): Tanh()\n",
       "    (input_dense): Linear(in_features=4, out_features=10, bias=True)\n",
       "    (hidden_dense): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (input_activation): ReLU()\n",
       "    (hidden_activation): ReLU()\n",
       "    (concat_dense): Linear(in_features=20, out_features=8, bias=True)\n",
       "    (z_activation): ReLU()\n",
       "  )\n",
       "  (decoder): decoder()\n",
       ")"
      ]
     },
     "execution_count": 882,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=1e-3\n",
    "model=VaeStar(input_dim=x_input.shape[1]-1,hidden_dims=[10,10],z_dim=4)\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20#draw_size #need to make sure everything adds up\n",
    "lr = 1e-3\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat=torch.cat([x_input,x_input_err],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values=torch.tensor(x_values)\n",
    "stacked_isochrones=torch.tensor(stacked_isochrones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "x_cat=DataLoader(x_cat.float(),batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 5, 32])\n",
      "flag\n",
      "12\n",
      "hello\n",
      "torch.Size([20, 10])\n",
      "torch.Size([20, 10])\n",
      "torch.Size([20, 20])\n",
      "flag2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected value argument (Tensor of shape (1, 1)) to be within the support (IndependentConstraint(Real(), 1)) of the distribution MultivariateNormal(loc: tensor([[0.]], dtype=torch.float64), covariance_matrix: tensor([[[inf]]], dtype=torch.float64)), but found invalid values:\ntensor([[nan]], dtype=torch.float64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/mattocallaghan/VaeStar/vaestar.ipynb Cell 32'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000024?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000024?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m12\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000024?line=14'>15</a>\u001b[0m kl, log_l, log_prob, z \u001b[39m=\u001b[39m model(x,x_err,x_values,stacked_isochrones)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000024?line=16'>17</a>\u001b[0m loss\u001b[39m=\u001b[39mkl\u001b[39m.\u001b[39msum()\u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39msum(log_l)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000024?line=17'>18</a>\u001b[0m overall_loss\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py:1357\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1351'>1352</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1352'>1353</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1353'>1354</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1354'>1355</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1355'>1356</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1356'>1357</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1357'>1358</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1358'>1359</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/mattocallaghan/VaeStar/vaestar.ipynb Cell 24'\u001b[0m in \u001b[0;36mVaeStar.forward\u001b[0;34m(self, x_input, x_input_err, x_values, all_isochrones)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000018?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mhello\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000018?line=46'>47</a>\u001b[0m z, z_ext,z_feh,z_age, kl\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x_input,x_input_err)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000018?line=47'>48</a>\u001b[0m \u001b[39mreturn\u001b[39;00m kl, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(z,all_isochrones,x_values)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py:1357\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1351'>1352</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1352'>1353</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1353'>1354</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1354'>1355</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1355'>1356</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1356'>1357</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1357'>1358</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1358'>1359</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/mattocallaghan/VaeStar/vaestar.ipynb Cell 24'\u001b[0m in \u001b[0;36mdecoder.forward\u001b[0;34m(self, z, all_isochrones, x_values)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000018?line=29'>30</a>\u001b[0m     isochrone\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcat((isochrone,x\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\u001b[39m1\u001b[39m,x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,(isochrone)\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]),x_err\u001b[39m.\u001b[39mreshape(x_err\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\u001b[39m1\u001b[39m,x_err\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,(isochrone)\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])),\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000018?line=31'>32</a>\u001b[0m     dist\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mmultivariate_normal\u001b[39m.\u001b[39mMultivariateNormal(torch\u001b[39m.\u001b[39mzeros_like(x),torch\u001b[39m.\u001b[39meye(\u001b[39mlen\u001b[39m(x))\u001b[39m+\u001b[39mtorch\u001b[39m.\u001b[39mdiag(x_err\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000018?line=32'>33</a>\u001b[0m     log_l\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mdist\u001b[39m.\u001b[39;49mlog_prob(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000018?line=33'>34</a>\u001b[0m     log_prob\u001b[39m.\u001b[39mappend(dist\u001b[39m.\u001b[39mlog_prob(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mattocallaghan/VaeStar/vaestar.ipynb#ch0000018?line=34'>35</a>\u001b[0m \u001b[39mreturn\u001b[39;00m log_l,log_prob,z\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py:214\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py?line=211'>212</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_prob\u001b[39m(\u001b[39mself\u001b[39m, value):\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py?line=212'>213</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_args:\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py?line=213'>214</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_sample(value)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py?line=214'>215</a>\u001b[0m     diff \u001b[39m=\u001b[39m value \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py?line=215'>216</a>\u001b[0m     M \u001b[39m=\u001b[39m _batch_mahalanobis(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unbroadcasted_scale_tril, diff)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py:294\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=291'>292</a>\u001b[0m valid \u001b[39m=\u001b[39m support\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=292'>293</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=293'>294</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=294'>295</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected value argument \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=295'>296</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=296'>297</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto be within the support (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(support)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=297'>298</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof the distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=298'>299</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/astro/lib/python3.8/site-packages/torch/distributions/distribution.py?line=299'>300</a>\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected value argument (Tensor of shape (1, 1)) to be within the support (IndependentConstraint(Real(), 1)) of the distribution MultivariateNormal(loc: tensor([[0.]], dtype=torch.float64), covariance_matrix: tensor([[[inf]]], dtype=torch.float64)), but found invalid values:\ntensor([[nan]], dtype=torch.float64)"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    overall_loss=0.0\n",
    "    for batch_idx,x in enumerate(x_cat):\n",
    "        x,x_err=torch.split(x,split_size_or_sections=int((x.shape[1]/2)),dim=1)\n",
    "        x=x.view(batch_size,x.shape[1],x.shape[2])\n",
    "        x_err=x_err.view(batch_size,x_err.shape[1],x_err.shape[2])\n",
    "        x=x.to(device)\n",
    "        x_err=x_err.to(device)\n",
    "        print(x.shape)\n",
    "        \n",
    "        print('flag')\n",
    "        optimizer.zero_grad()\n",
    "        print('12')\n",
    "        kl, log_l, log_prob, z = model(x,x_err,x_values,stacked_isochrones)\n",
    "\n",
    "        loss=kl.sum()-torch.sum(log_l)\n",
    "        overall_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss / (batch_idx*batch_size))\n",
    "        print(\"Overall Loss: \", overall_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bb95e0f03a712b005f38a08e86b3d9ee1481e5da27c42212222567f4ca651c8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('astro')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
